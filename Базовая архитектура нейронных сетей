Базовая архитектура многослойных НС — сети прямого распространения
(Feed-Forward Networks, FNN), где информация передается в одном направлении: от
входного слоя через скрытые слои к выходному слою. В таких НС нет циклов или
обратных связей, что делает их простыми для понимания и реализации.
Искусственные нейронные сети (или нейронные сети, НС) — модели ML, в основе
функционирования которых лежат принципы работы биологических нейронов в
человеческом мозге.
Идея, лежащая в основе НС: каждый биологический нейрон имеет несколько входов
(дендритов), на основе информации с которых формируется выходной сигнал, который
с помощью выхода (аксона) передается далее к органам человеческого (и не только)
организма.
Для каждого текущего нейрона аксоны входных нейронов являются синапсами Аксон
текущего нейрона — синапс выходного нейрона Нейроны одного уровня образуют слой
Обучение нейронной сети сводится к настройке весов синаптических каналов
Многослойный перцептрон (MLP): Концепция многослойного перцептрона, как мы
знаем её сегодня, была развита позже. В 1980-х гг Джейфри Хинтон, Д. Румельхарт и
другие представили методы обучения многослойных НС с использованием алгоритма
обратного распространения ошибки (backpropagation). Это позволило эффективно
обучать сети с несколькими скрытыми слоями.
Многослойный перцептрон — одна из наиболее простых архитектур НС, с которых
обычно начинается работа с НС.

Перцептронт Розенблатта:
1. Использование нелинейной активационной функции: Перцептрон Розенблатта
использует линейную активационную функцию (например, пороговую функцию),
что ограничивает его возможности в решении задач с нелинейными границами.
Перцептрон Румельхарта (или MLP) использует нелинейные активационные
функции (например, сигмоидную или ReLU), что позволяет ему моделировать
более сложные функции.
2. Число скрытых слоёв более одного: Перцептрон Розенблатта состоит только из
одного слоя (входной и выходной), тогда как многослойный перцептрон
Румельхарта может иметь несколько скрытых слоёв, что значительно
увеличивает его способность к обучению.
3. Входные сигналы не бинарные: Перцептрон Розенблатта работает с бинарными
входами (0 или 1), тогда как в многослойном перцептроне Румельхарта входные
данные могут быть представлены в виде непрерывных значений, например,
нормированных чисел в интервале [0, 1].
4. Выходная ошибка определяется как некоторое значение невязки: В
многослойном перцептроне ошибка часто определяется как функция потерь
(например, среднеквадратичная ошибка), измеряющая невязку между
предсказанными и истинными значениями, в отличие от простого подсчёта
ошибочно классифицированных примеров в перцептроне Розенблатта.
5. Обучение производится не до минимизации ошибки, а до стабилизации весов:
В процессе обучения многослойного перцептрона Румельхарта действительно
может происходить стабилизация весов, чтобы избежать переобучения. Это
может включать использование методов регуляризации и ранней остановки, в
то время как в классическом перцептроне Розенблатта обучение продолжается
до достижения определённого уровня ошибок.
Многослойный перцептрон содержит несколько вычислительных слоев.
Дополнительные промежуточные слои (расположенные между входом и выходом) —
скрытые слои, т.к. выполняемые ими вычисления остаются невидимыми для
пользователя.
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
# Загружаем данные и делим на train/test
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
# Создаём многослойный персептрон с одним скрытым слоем из 100 нейронов
model = MLPClassifier(hidden_layer_sizes=(100,),
activation='relu',
solver='adam',
learning_rate_init=0.001,
max_iter=200,
random_state=42)
# Обучаем и предсказываем
model.fit(X_train, y_train)
predictions = model.predict(X_test)
# Оценка точности
accuracy = model.score(X_test, y_test)
