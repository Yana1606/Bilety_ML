Нелинейные методы снижения размерности
Снижение размерности служит для визуализации высокоразмерных данных,
ускорения обучения моделей и подавления шума. В тех случаях, когда структура
данных не укладывается в линейные подпространства, применяют нелинейные
техники, сохраняющие сложные геометрические и топологические свойства исходного
пространства.
t-SNE (t-distributed Stochastic Neighbor Embedding) основывается на вероятностном
подходе: в исходном пространстве каждая пара точек связывается условной
вероятностью близости, пропорциональной гауссову ядру, а в целевом
низкоразмерном пространстве сходную вероятность приближают с помощью
распределения Стьюдента с одной степенью свободы. Алгоритм минимизирует
расхождение двух распределений (KL-дивергенцию), что позволяет отобразить схожие
объекты как кластеры и оттенить локальную структуру. Однако t-SNE неявно не
сохраняет глобальные расстояния, плохо масштабируется на сотни тысяч объектов и
требует тщательного подбора параметров, таких как «перплекcия» и скорость
обучения.
Isomap сочетает классический метод многомерного шкалирования (MDS) с графом
ближайших соседей. Сначала строится граф, в котором соседями считаются точки в
пределах заданного числа k или радиуса ε, затем между любой парой точек
вычисляется кратчайший путь по этому графу, приближая геодезические расстояния на
многообразии данных. На этой основе классический MDS восстанавливает координаты
в низкоразмерном пространстве. Таким образом Isomap лучше сохраняет глобальную
геометрию и может выявлять «скрученные» структуры, но чувствителен к выбору
числа соседей, а в разреженных областях графа кратчайшие пути могут искажать
реальную геометрию.
UMAP (Uniform Manifold Approximation and Projection) опирается на математические
основания алгебраической топологии и теории многообразий. Как и t-SNE, он строит
локальные графы близости, но в качестве меры близости использует взвешенные
ряды вероятностей на основе экспоненциального ядра, а при проекции минимизирует
кросс-энтропию между высокоразмерной и низкоразмерной структурами графов.
Благодаря оптимальному компромиссу между сохранением локальной и глобальной
информации UMAP демонстрирует более высокую производительность и
масштабируемость, чем t-SNE, и зачастую лучше раскрывает макро- и микроструктуры
данных.
Каждый из этих методов имеет свои сильные и слабые стороны. Визуализация t-SNE
часто более выразительна в локальном разбиении, но менее стабильна при повторных
запусках. Isomap дает более физически обоснованное отображение глобальных
геодезических расстояний, однако страдает от проблем связности графа. UMAP
объединяет достоинства обеих подходов, оставаясь достаточно быстрым и
универсальным.
Методы выбора признаков
В отличие от снижения размерности, при котором признаки преображаются и теряют
физический смысл, выбор признаков сохраняет исходные переменные, что важно для
задач, требующих объяснимости результатов. Цель отбора — оставить только
информативные признаки, улучшить обобщающую способность модели, ускорить
обучение и снизить риск переобучения.
Существует три основных подхода к выбору признаков. Фильтрационные методы
оценивают значимость каждого признака независимо от модели. Они используют
статистические показатели: коэффициенты корреляции, тесты на значимость,
дисперсионный анализ. Предварительно удаляются признаки с близкой к нулю
дисперсией, так как они не вносят информации. Избыточность выявляют по высокой
парной корреляции: при значении коэффициента Пирсона выше девяти десятых один
из пары удаляют, чтобы снизить многоколлинеарность. Для категориальных данных
применяют критерий Хи-квадрат, для монотонных зависимостей — ранговый
коэффициент Спирмена, а для сравнения средних нескольких групп — ANOVA и
F-тест.
Встроенные методы интегрируют отбор прямо в процесс обучения модели. Наиболее
распространённым примером является регуляризация L1 в линейных моделях, при
которой в функцию потерь добавляется сумма абсолютных значений коэффициентов.
Алгоритм стремится одновременно минимизировать ошибку и абсолютную сумму
весов, что приводит к обнулению неинформативных коэффициентов. В решающих
деревьях и ансамблях (Random Forest, градиентный бустинг) важность признаков
вычисляется на основе суммарного уменьшения критерия разбиения при разделениях
по данным признакам. Такой подход требует меньше вычислений, чем обёртки, и
учитывает взаимодействия признаков в рамках конкретной модели.
Методы-оболочки (wrapper) оценивают качество подмножества признаков по
результатам работы конкретного алгоритма. Они подбирают комбинацию переменных
итеративно, обучая модель на разных конфигурациях: «жадный» прямой или обратный
отбор, пошаговый отбор, а также более продвинутые стратегии, такие как генетические
алгоритмы. За счёт учёта взаимодействий признаков wrapper-методы зачастую
достигают более высокой точности, но обходятся дорогим перебором и требуют
значительных вычислительных ресурсов.
Выбор между этими подходами зависит от задач. Если важно быстро отсеять «шума» и
получить простую оценку информативности каждого признака, подойдут методы
фильтрации. Когда модель должна сама выявить значимые переменные в процессе
обучения, разумно применять встроенные подходы. Если приоритетом является
максимальная точность и допустимы затраты вычислительных ресурсов, оправдано
использование wrapper-методов. На практике часто комбинируют несколько стратегий:
сначала фильтрация для грубой очистки и снижения размерности, затем встроенный
или wrapper-отбор для уточнения списка признаков.
