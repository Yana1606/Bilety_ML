Глубокое обучение (Deep Learning, DL) — подход к ML, который основывается на использовании
многослойных НС для автоматического извлечения признаков из сложных
(нелинейных) данных.

Цель глубокого обучения: построить модель, которая сможет автоматически изучать
сложные (нелинейные) закономерности на основе данных и делать точные прогнозы
или решения.

Искусственные нейронные сети (или нейронные сети, НС) — модели ML, в основе
функционирования которых лежат принципы работы биологических нейронов в
человеческом мозге.

Идея, лежащая в основе НС: каждый биологический нейрон имеет несколько входов
(дендритов), на основе информации с которых формируется выходной сигнал, который
с помощью выхода (аксона) передается далее к органам человеческого (и не только)
организма.

Модель формального нейрона состоит из трех основных компонент: СИНАПСЫ — это
входные сигналы, каждый из которых имеет свой собственный вес , отражающий силу
связи между двумя нейронами. СУММАТОР — выполняет сложение входных сигналов
предварительно помноженных на соответствующие (синаптические) веса .
ПРЕОБРАЗОВАТЕЛЬ реализует функцию одного аргумента — выход сумматора. Эта
функция называется ФУНКЦИЕЙ АКТИВАЦИИ (или ПЕРЕДАТОЧНАЯ ФУНКЦИЯ
НЕЙРОНА). Данный компонент ограничивает амплитуду выходного сигнала, в
результате чего выход нейрона, как правило, лежит в диапазоне или . При
необходимости он усиливает слабые сигналы и ослабляет сильные сигналы.

Функция активации — математическая функция, которая применяется к выходным
данным каждого нейрона в НС.
· Функции активации способны контролировать широкий диапазон значений
нейронов.
· Функции активации позволяют НС изучать нелинейные зависимости и
сложные закономерности в данных, что способствует более точным
прогнозам.
· Основное свойство функции активации: ее нелинейность.

Задача функции активации — помочь нейронам принимать локальные решения.

Полносвязная нейронная сеть (Fully Connected Neural Network, FCNN) — это базовый тип 
искусственной нейронной сети, где каждый нейрон одного слоя соединён со всеми нейронами следующего слоя.

Основные компоненты FCNN
- Входной слой (Input Layer) - принимает исходные данные и количество нейронов равно размерности входных данных.
- Скрытые слои (Hidden Layers) - промежуточные слои между входом и выходом; каждый нейрон применяет линейное преобразование 
(взвешенная сумма входов + смещение) и нелинейную функцию активации.
- Выходной слой (Output Layer) - формирует итоговый результат; функция активации зависит от задачи (Softmax для классификации, Sigmoid/Linear для регрессии).

Понятие бэтча и эпохи
Когда мы тренируем нейронную сеть, у нас почти всегда очень большой объём
данных, поэтому сразу прогонять всю выборку через модель бывает и долго, и
неэффективно. Вместо этого dataset разбивают на маленькие порции, которые
называют бэтчами (batch). За один шаг обучения (one update) сеть видит именно один
бэтч и на его основе вычисляет градиент и делает корректировку весов. Размер бэтча
влияет на стабильность и скорость обучения: маленькие бэтчи дают более шумные, но
частые обновления, большие — более гладкие, но редкие.

Эпоха (epoch) — это один полный проход по всем данным, то есть когда каждая
образец из датасета один раз оказался в каком-то бэтче и был использован для
обновления модели. Обычно требуется несколько эпох: после первой эпохи сеть уже
умеет что-то «видеть», но переобучается она и дообучается в следующих. Каждая
новая эпоха позволяет скорректировать веса с учётом информации, которую модель
смогла усвоить на предыдущих проходах по данным.

Функции ошибки и обучение с помощью обратного распространения градиента
Чтобы сеть училась, нам нужно понять, насколько её предсказание отличается от
желаемого ответа. Это измеряет функция ошибки (loss function). Для регрессии часто
используют среднеквадратичную ошибку (MSE), для задач классификации —
кросс-энтропию. Функция ошибки выдаёт число, показывающее «стоимость» текущих
весов сети на данной выборке.

Далее вступает в дело алгоритм обратного распространения ошибки (backpropagation).
Сначала вычисляют прямой проход: входные данные проходят через все слои, и на
выходе получается предсказание. Затем считают ошибку по выбранной функции.
После этого алгоритм автоматически распространяет это значение обратно по всем
слоям, вычисляя производные функции ошибки по каждому весу. Градиент ошибки
показывает направление, в котором надо изменить вес, чтобы уменьшить ошибку.

Наконец, модель обновляет веса шагом градиентного спуска: из каждого веса
вычитается небольшая величина, пропорциональная этому градиенту и выбранной
скорости обучения (learning rate). Так повторяется для каждого бэтча, внутри каждой
эпохи: прямая передача, вычисление ошибки, обратное распространение, обновление
весов. С каждым таким шагом сеть постепенно «спускается» по поверхности ошибки к
минимуму и тем самым учится правильно решать поставленную задачу
