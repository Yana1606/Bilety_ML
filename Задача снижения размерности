ЗАДАЧА ПОНИЖЕНИЯ РАЗМЕРНОСТИ -одна из важнейших в анализе данных и
ML,может возникнуть в двух следующих случаях.
1. Визуализация: при работе с многомерными данными необходимо посмотреть на
их структуру, уменьшив размерность.
2. Предобработка признаков в моделях ML: неудобно обучать алгоритмы на сотне
признаков, среди которых может быть множество зашумленных и/или линейно
зависимых (от них лучше избавиться). ]
СНИЖЕНИЕ РАЗМЕРНОСТИ ДАННЫХ – семейство методов, используемых в анализе
и моделировании данных для уменьшения сложности данных при максимальном
сохранении их исходной информативности. Это важнейший инструмент в науке о
данных, особенно при работе с большими высокоразмерными наборами данных.
УМЕНЬШЕНИЕ РАЗМЕРНОСТИ – уменьшение числа признаков набора данных.
1. Снижение времени обучения, экономия вычислительных ресурсов для
настройки и функционирования модели ML.
2. Снижение переобучения модели ML.
3. Полезно для визуализации данных.
4. Решение проблемы мультиколлинеарности. Между нецелевыми
признаками могут существовать зависимости — многие модели
предсказания в этой ситуации работают плохо (например, линейная
регрессия, Наивный Байес)
5. Уменьшение шумов в данных.
6. Сжатие данных. Когда много признаков, то труднее найти закономерность.
7. Если признаков много, то возникает «проклятие размерности». Линейные
и метрические модели становятся неэффективными.
8. Преобразует линейно-неразделимые данные в линейно-разделимую
форму.
9. Помогает при нахождении скрытых переменных.
Метод главных компонент (PCA) — один из основных
способов в ML уменьшить размерность данных, потеряв наименьшее количество
информации. ГЛАВНЫЕ КОМПОНЕНТЫ — новые переменные, с помощью которых
будут описывать объекты. 
МЕТОД ГЛАВНЫХ КОМПОНЕНТ
1. Стандартизация данных.
2. Вычисление ковариационной матрицы.
3. Вычисление собственных векторов и собственных значений
ковариационной матрицы.
4. Сортировка пар <собственное значение, собственный вектор> по
убыванию. Чем больше число, тем больше дисперсия.
5. Выбор первых k пар <собственное значение, собственный вектор>, где k
— размерность целевого пространства.
6. Матрица, составленная из k собственных векторов — матрица
преобразования из данного пространства в пространство с размерностью k.
Берем N первых собственных векторов, которые соответствуют первым N
собственным числам. Это и есть искомые главные компоненты.
7. Чтобы произвести понижение размерности следует умножить матрицу
стандартизованных входных данных (результат п. 1) на матрицу из k
собственных векторов (результат п. 6).
Ядровой PCA — это способ «уловить» в данных скрытые, изогнутые формы, когда
обычный PCA не справляется. Представьте себе, что вы бросаете резинку на гвозди,
воткнутые в доску так, что она образует сложный рисунок: в обычном PCA вы
пытаетесь расправить этот рисунок прямо на доске, и получается лишь грубая линия,
не повторяющая изгибы. А ядровой метод похож на то, как если бы вы подняли доску
вверх, растянули резинку в трёхмерном пространстве, расправили её там, а потом
снова аккуратно опустили — контур на доске теперь становится более «прямым» и
поэтому проще поддаётся анализу.
То есть вместо того чтобы пытаться найти прямые направления сразу в исходных
данных, мы «переносим» точки в более богатое пространство, где их форма
становится линейнее. Но при этом нам не нужно описывать это новое пространство
явно — мы лишь умеем «замерять» расстояния и близость между точками так, как
если бы они уже там находились. Это и называется «ядром».
Благодаря такому приёму ядровой PCA способен лучше разделять группы объектов и
выявлять «скрученные» структуры — например, когда похожие точки формируют
спираль или другую необычную форму. Он особенно полезен в тех случаях, когда
нужно сохранить сложную геометрию данных, а не просто сжать их до нескольких
координат линейным способом.
Главные преимущества ядрового подхода в том, что он легко адаптируется к разным
задачам за счёт выбора «меры близости» (ядра), а недостаток — это возросшая
вычислительная нагрузка и необходимость подбора параметров ядра. Поэтому на
практике его применяют, когда важно получить более точную низкоразмерную картину
сложных данных, и когда есть ресурсы для экспериментов с настройками.
